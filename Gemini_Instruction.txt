1. backend\.env

You have to play along the below lines :

    # LLM runtime selection: ollama | vllm | gemini
    LLM_RUNTIME=gemini

    # Gemini (Google Cloud)
    GEMINI_API_KEY = 

    # Ollama (local). Do NOT expose to network unless necessary.
    #OLLAMA_URL=http://localhost:11434
    #MODEL_NAME=qwen2.5:3b

==================================================================

2. backend/app/core/config.py

comment the below when using ollama model

    GEMINI_API_KEY: str = os.getenv("GEMINI_API_KEY", "") # <<<<< NEW

==================================================================

3. backend/app/core/model_registry.py

ppl using 7b parameter model, change this line of code acc
    if runtime == "ollama":
        name = os.getenv("MODEL_NAME", "qwen2.5:3b")#"qwen2.5:7b-instruct-q4_K_M" <---- THIS LINE
        cfg = _OLLAMA_REGISTRY.get(name)
        if not cfg:
            raise ValueError(f"MODEL_NAME '{name}' not found in registry. Available: {list(_OLLAMA_REGISTRY.keys())}")

        return cfg